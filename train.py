#!/usr/bin/env python
import functools
import jax, jax.numpy as jnp, jax.random as jr, tqdm
from model import Miniformer
from tokenizer import Bpe
import dataset, checkpoint

key = jr.key(1337)

# if the JSON file is not available, it can be generated by running the "tokenizer.py" script
with open("models/tokenizer.json") as f:
  tok = Bpe.load(f)

vocab_size = len(tok.vocab)
text = dataset.raw()

# create dataset from text
dataset = jnp.array(tok.encode(text))
thres = int(.9 * len(dataset))
Xtr, Xte = dataset[:thres], dataset[thres:]
len(Xtr), len(Xte)

def get_batch(key, sample_len, batch_size, split="train"):
  dataset = {"train": Xtr, "test": Xte}[split]
  start_ix = jr.randint(key, (batch_size,), 0, len(dataset) - sample_len + 1)
  return jax.vmap(jax.lax.dynamic_slice_in_dim, (None,0,None))(dataset, start_ix, sample_len)

# instantiate model
L = 25
model = Miniformer.from_spec(5, vocab_size, 50, 4, 100)
key, subkey = jr.split(key)
params = model.init(subkey)
num_params = sum(jax.tree.flatten(jax.tree.map(lambda p: p.size, params))[0])
print(f"num params: {num_params / 1e3:.02f}k")

@functools.partial(jax.jit, static_argnames=["model", "split", "batch_size"])
def acc(model, params, key, split, batch_size=1000):
  batch = get_batch(key, L + 1, batch_size, split=split)
  x, y = batch[:,:-1], batch[:,1:]
  return jnp.mean(model(params, x).argmax(-1) == y)

def loss(model, params, x, y):
  y_pred = model(params, x)
  logits = jax.nn.log_softmax(y_pred)
  return -jnp.mean(jnp.sum(jax.nn.one_hot(y, vocab_size) * logits, axis=-1))

@functools.partial(jax.jit, static_argnames=["model"])
def update_step(model, params, key):
  # generate a batch of data
  batch_size = 250
  batch = get_batch(key, L + 1, batch_size)
  x, y = batch[:,:-1], batch[:,1:]
  # compute loss and gradients
  lossval, grads = jax.value_and_grad(loss, argnums=1)(model, params, x, y)
  # update parameters
  return lossval, jax.tree.map(lambda p, g: p - 5e-2 * g, params, grads)

# run training
num_epochs = 50000
for epoch in (pbar := tqdm.trange(1, 1 + num_epochs)):
  # run update
  key, subkey = jr.split(key)
  lossval, params = update_step(model, params, subkey)

  # bookkeeping via progress bar
  if (epoch % 1000) == 0:
    stats = {"NLL": lossval.item()}
    key, subkey = jr.split(key)
    for short, split in (("Atr", "train"), ("Ate", "test")):
      stats[short] = acc(model, params, subkey, split).item()
    pbar.set_description(" ".join(f"%s: %.02f" % tup for tup in stats.items()))

# save model
checkpoint.save(model, params)
