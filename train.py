#!/usr/bin/env python
import functools
import jax, jax.numpy as jnp, jax.random as jr, tqdm
from model import Miniformer
from tokenizer import Bpe
import dataset, checkpoint, config

# get experiment configuration
hparams = config.get_config()
key = jr.key(1337)

# if the JSON file is not available, it can be generated by running the "tokenizer.py" script
with open(hparams.tokenizer_json) as f:
  tok = Bpe.load(f)
text = dataset.raw()

# create dataset from text
dataset = jnp.array(tok.encode(text))
thres = int(hparams.train_test_split_fraction * len(dataset))
Xtr, Xte = dataset[:thres], dataset[thres:]
len(Xtr), len(Xte)

def get_batch(key, sample_len, batch_size, split="train"):
  dataset = {"train": Xtr, "test": Xte}[split]
  start_ix = jr.randint(key, (batch_size,), 0, len(dataset) - sample_len + 1)
  return jax.vmap(jax.lax.dynamic_slice_in_dim, (None,0,None))(dataset, start_ix, sample_len)

# instantiate model
model = Miniformer.from_spec(hparams.seq_len, hparams.num_blocks, tok.num_tokens, hparams.emb_dim, hparams.num_heads, hparams.hidden_dim)
key, subkey = jr.split(key)
params = model.init(subkey)
num_params = sum(jax.tree.flatten(jax.tree.map(lambda p: p.size, params))[0])
print(f"num params: {num_params / 1e3:.02f}k")

@functools.partial(jax.jit, static_argnames=["model", "split", "subset_size"])
def acc(model, params, key, split, subset_size=1000):
  batch = get_batch(key, model.seq_len + 1, subset_size, split=split)
  x, y = batch[:,:-1], batch[:,1:]
  return jnp.mean(model(params, x).argmax(-1) == y)

def loss(model, params, x, y):
  y_pred = model(params, x)
  logits = jax.nn.log_softmax(y_pred)
  return -jnp.mean(jnp.sum(jax.nn.one_hot(y, tok.num_tokens) * logits, axis=-1))

@functools.partial(jax.jit, static_argnames=["model"])
def update_step(model, params, key):
  # generate a batch of data
  batch = get_batch(key, model.seq_len + 1, hparams.batch_size)
  x, y = batch[:,:-1], batch[:,1:]
  # compute loss and gradients
  lossval, grads = jax.value_and_grad(loss, argnums=1)(model, params, x, y)
  # update parameters
  return lossval, jax.tree.map(lambda p, g: p - hparams.learning_rate * g, params, grads)

# run training
for epoch in (pbar := tqdm.trange(1, 1 + hparams.num_epochs)):
  # run update
  key, subkey = jr.split(key)
  lossval, params = update_step(model, params, subkey)

  # bookkeeping via progress bar
  if (epoch % 1000) == 0:
    stats = {"NLL": lossval.item()}
    key, subkey = jr.split(key)
    for short, split in (("Atr", "train"), ("Ate", "test")):
      stats[short] = acc(model, params, subkey, split).item()
    pbar.set_description(" ".join(f"%s: %.02f" % tup for tup in stats.items()))

# save model
if hparams.save_dir is not None:
  checkpoint.save(model, params, save_dir=hparams.save_dir)
